{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f3e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import sklearn.manifold\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f661d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KruskalStress:\n",
    "    \n",
    "    def __init__(self, full_matrix, reduced_matrix, k = 100, method = 'd'):\n",
    "        \n",
    "        self.k = k\n",
    "        self.method = method\n",
    "        self.full_matrix = full_matrix\n",
    "        self.reduced_matrix = reduced_matrix\n",
    "        self.value_of_stress = self.kruskal_stress()\n",
    "        \n",
    "    def create_distance_matrix(self, data_set):\n",
    "        samples = data_set.shape[0]\n",
    "        d = pdist(data_set)\n",
    "        distances = {}\n",
    "        for i in range(samples - 1):\n",
    "            distances[i] = {i + n + 1: d for n, d in\n",
    "                            enumerate(d[0:samples - i - 1])}\n",
    "            d = d[samples - i - 1:]\n",
    "        return distances\n",
    "    \n",
    "    def k_nearest(self, data_set):\n",
    "        distance_matrix = self.create_distance_matrix(data_set)\n",
    "        nodes = len(distance_matrix) + 1\n",
    "        result = dict()\n",
    "        for v in range(nodes):\n",
    "            candidate_links = [distance_matrix[n][v] for n in range(0, v)] + [0] + [distance_matrix[v][n] for n in range(v + 1, nodes)]\n",
    "            closest_neighbors = np.argsort(candidate_links)\n",
    "            closest_neighbors = closest_neighbors[closest_neighbors != v][:self.k]\n",
    "            result[v] = dict()\n",
    "            for neighbor in closest_neighbors:\n",
    "                _min, _max = min(v, neighbor), max(v, neighbor)\n",
    "                result[_min][_max] =distance_matrix[_min][_max]\n",
    "        return result\n",
    "    \n",
    "    def creating_shortest_path(self, data_set):\n",
    "        k_nearest_matrix = self.k_nearest(data_set)\n",
    "        g = nx.Graph()\n",
    "        for v, edges in k_nearest_matrix.items():\n",
    "            g.add_weighted_edges_from(((v, n, cost) for n, cost in edges.items()))\n",
    "        if self.method == 'd':\n",
    "            answer = dict(nx.all_pairs_dijkstra_path_length(g))\n",
    "        else:\n",
    "            answer = dict(nx.floyd_warshall(g))\n",
    "        return answer\n",
    "    \n",
    "    def creating_dissimilarity_matrix(self, data_set):\n",
    "        short_path_dict = self.creating_shortest_path(data_set)\n",
    "        dissimilarities = np.zeros((data_set.shape[0], data_set.shape[0]))\n",
    "\n",
    "        for node, edges in short_path_dict.items():\n",
    "            for neighbor, dissimilarity in edges.items():\n",
    "                dissimilarities[node, neighbor] = dissimilarity\n",
    "        return dissimilarities\n",
    "    \n",
    "    def kruskal_stress(self):\n",
    "        dissimilarity_matrix_x = self.creating_dissimilarity_matrix(self.full_matrix)\n",
    "        dissimilarity_matrix_y = self.creating_dissimilarity_matrix(self.reduced_matrix)\n",
    "        return np.sqrt(np.power(dissimilarity_matrix_x - dissimilarity_matrix_y, 2).sum() / np.power(dissimilarity_matrix_x, 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376dabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle():\n",
    "    def __init__(self, n_dim, n_features_to_consider, data_matrix):\n",
    "        self.n_dim = n_dim\n",
    "        self.n_feat = n_features_to_consider\n",
    "        self.data = data_matrix\n",
    "        self.position_i = []\n",
    "        self.velocity_i = []\n",
    "        self.pos_best_i = []\n",
    "        self.err_best_i = -1\n",
    "        self.err_i = -1\n",
    "    \n",
    "        for i in range(0, self.n_dim):\n",
    "            self.velocity_i.append(random.uniform(0,1))\n",
    "            self.position_i.append(random.uniform(0,1))\n",
    "\n",
    "    def evaluate(self,costFunc):\n",
    "        indices = np.argsort(self.position_i)[-self.n_feat:]\n",
    "        reduced_matrix = self.data.iloc[:,indices]\n",
    "        self.err_i=costFunc(self.data, reduced_matrix).value_of_stress\n",
    "        \n",
    "        if self.err_i < self.err_best_i or self.err_best_i==-1:\n",
    "            self.pos_best_i=self.position_i\n",
    "            self.err_best_i=self.err_i\n",
    "    \n",
    "    def update_velocity(self,pos_best_g):\n",
    "        w=0.5       # constant inertia weight\n",
    "        c1=1        # cognative constant\n",
    "        c2=2        # social constant\n",
    "\n",
    "        for i in range(0,self.n_dim):\n",
    "            r1=random.random()\n",
    "            r2=random.random()\n",
    "\n",
    "            vel_cognitive=c1*r1*(self.pos_best_i[i]-self.position_i[i])\n",
    "            vel_social=c2*r2*(pos_best_g[i]-self.position_i[i])\n",
    "            self.velocity_i[i]=w*self.velocity_i[i]+vel_cognitive+vel_social\n",
    "    \n",
    "    def update_position(self):\n",
    "        for i in range(0,self.n_dim):\n",
    "            self.position_i[i]=self.position_i[i]+self.velocity_i[i]\n",
    "\n",
    "            # adjust maximum position if necessary\n",
    "            if self.position_i[i]>1:\n",
    "                self.position_i[i]=0.9\n",
    "\n",
    "            # adjust minimum position if neseccary\n",
    "            if self.position_i[i] < 0:\n",
    "                self.position_i[i]=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b32c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO():\n",
    "    def __init__(self, costFunc, n_dim, n_feat, data_set, num_particles, maxiter):\n",
    "        \n",
    "        err_best_g=-1                   # best error for group\n",
    "        pos_best_g=[]                   # best position for group\n",
    "\n",
    "        self.result_list = []\n",
    "        \n",
    "        swarm=[]\n",
    "        for i in range(0,num_particles):\n",
    "            swarm.append(Particle(n_dim, n_feat, data_set))\n",
    "\n",
    "        # begin optimization loop\n",
    "        i=0\n",
    "        while i < maxiter:\n",
    "            t1 = datetime.now()\n",
    "            for j in range(0,num_particles):\n",
    "                swarm[j].evaluate(costFunc)\n",
    "\n",
    "                if swarm[j].err_i < err_best_g or err_best_g == -1:\n",
    "                    pos_best_g=list(swarm[j].position_i)\n",
    "                    err_best_g=float(swarm[j].err_i)\n",
    "#                     print(err_best_g)\n",
    "            t2 = datetime.now()\n",
    "            delta_t = t2-t1\n",
    "            self.result_list.append((n_feat, i, pos_best_g, err_best_g, delta_t))\n",
    "\n",
    "            for j in range(0,num_particles):\n",
    "                swarm[j].update_velocity(pos_best_g)\n",
    "                swarm[j].update_position()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a12bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = #number-of-iterations\n",
    "n_particles = #number-of-particles\n",
    "number_of_features = #initial-number-of-features\n",
    "n_features = #desired-number-of-features\n",
    "repo = #matrix representing the repository\n",
    "\n",
    "results = [pool.apply(PSO, args=row) for row in [(KruskalStress, number_of_features, n_features, \n",
    "                                                          repo, n_particles, n_iterations)]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
